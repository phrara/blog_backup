---
title: K8s(1)
tags:
  - docker
  - 容器
categories:
  - 云计算
  - K8s
abbrlink: 8e372fbf
---

# 主机网络模式探究

> Pod为Host网络模式，即直接使用宿主机的网络，不进行网络虚拟化隔离。这样一来，Pod中的所有容器就直接暴露在宿主机的网络环境中，这时候，Pod的PodIP就是其所在Node的IP。
> 从原理上来说，当设定Pod的网络为Host时，是设定了Pod中pod-infrastructure（或pause）容器的网络为Host，Pod内部其他容器的网络指向该容器。

## Yaml
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 1
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: mynginx
        imagePullPolicy: IfNotPresent # 宿主机不存在则拉取远程
        image: nginx
      hostNetwork: true
```

## k8s pod镜像拉取策略
+ Always：每次创建pod时重新远程拉取镜像
+ IfNotPresent：宿主机不存在则拉取远程
+ Never：只使用宿主机上存在的，永不拉取

## Command
```bash
# 根据yaml配置文件创建资源
kubectl create -f xxx.yml

# 资源本不存在: 创建新资源。
# 资源已存在: 使配置在已存在的资源上生效
kubectl apply -f xxx.yml

# 获取某类资源列表信息
kubectl get pod/deployment/...
# 删除某类资源
kubectl delete pod/deployment/...
# 详细信息
kubectl describe pod
# 打印日志
kubectl logs

```

## 常见错误

![](./K8s(1)/err0.png)
+ ImagePullBackOff：远程镜像拉取失败
+ ErrImageNeverPull：当策略为Never且部署节点上不存在该镜像；从节点上要有这个镜像。**准确的说是调度到哪个节点，那个节点上就要有这个镜像**，否则会报错；*当使用本地镜像时需要注意该问题！*

![](./K8s(1)/err1.png)
![](./K8s(1)/err2.png)
已pod形式启动了一个镜像，执行kubectl get pod 查看状态，发现pod一直重启。查看具体的报错信息，发现报错内容为：Back-off restarting failed container
+ 原因：镜像启动容器后，**容器内部没有常驻进程，导致容器启动成功后即退出**，从而进行了持续的重启。
+ 解决：只需要给容器加上一个常驻的进程就可以要使Pod持续运行，就必须指定一个永远不会完成的任务。
因此在yaml文件中指定一个启动命令，完整内容如下:
```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 1
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: mynginx
        imagePullPolicy: IfNotPresent # 宿主机不存在则拉取远程
        image: nginx
        command: ["/bin/bash", "-ce", "tail -f /dev/null"]
      hostNetwork: true
```


# K8s防护调研

## KubeArmor

>KubeArmor是一个支持[容器](https://cloud.tencent.com/product/tke?from=20065&from_column=20065)的运行时安全实施系统，它可以从系统级别限制容器的行为（如进程执行、文件访问、网络操作和资源利用率）。

[官网](https://kubearmor.io/)
[git](https://github.com/kubearmor/KubeArmor) 

该工具可通过规则配置，对容器指定资源与行为进行监控和拦截，例如：
```yaml
apiVersion: security.kubearmor.com/v1
kind: KubeArmorPolicy
metadata:
  name: ksp-group-1-proc-path-block
  namespace: multiubuntu
spec:
  selector:
    matchLabels:
      group: group-1
  process:
    matchPaths:
    - path: /bin/sleep
  action:
    Block
```
说明：此策略的目的是阻止在带有“group-1”标签的容器中执行“/bin/sleep”。 为此，我们在 selector -> matchLabels 中定义 'group-1' 标签，在 process -> matchPaths 中定义特定路径（'/bin/sleep'）。 此外，我们将“阻止”作为此策略的操作。
通过不同的规则配置可达到不同的拦截效果，下面列举出部分：
+ Block a specific executable
+ Block all executables in a specific directory
+ Block all executables in a specific directory and its subdirectories
+ Allow specific executables to access certain files only
+ .....

## NeuVector

>NeuVector提供实时深入的容器网络可视化、东西向容器网络监控、主动隔离和保护、容器主机安全以及容器内部安全。
>NeuVector并非某个组件或者安全工具，而是一套完整的容器安全平台。

[Install](https://www.cnblogs.com/panlifeng/p/16326578.html)

### Install
创建 namespace

```cpp
kubectl create namespace neuvector
```

部署 CRD (Kubernetes 1.19+ 版本)

```ruby
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.19.yaml
```

部署 CRD (Kubernetes 1.18 或更低版本)

```ruby
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/crd-k8s-1.16.yaml
```

配置 RBAC

```sql
kubectl create clusterrole neuvector-binding-app --verb=get,list,watch,update --resource=nodes,pods,services,namespaces
kubectl create clusterrole neuvector-binding-rbac --verb=get,list,watch --resource=rolebindings.rbac.authorization.k8s.io,roles.rbac.authorization.k8s.io,clusterrolebindings.rbac.authorization.k8s.io,clusterroles.rbac.authorization.k8s.io
kubectl create clusterrolebinding neuvector-binding-app --clusterrole=neuvector-binding-app --serviceaccount=neuvector:default
kubectl create clusterrolebinding neuvector-binding-rbac --clusterrole=neuvector-binding-rbac --serviceaccount=neuvector:default
kubectl create clusterrole neuvector-binding-admission --verb=get,list,watch,create,update,delete --resource=validatingwebhookconfigurations,mutatingwebhookconfigurations
kubectl create clusterrolebinding neuvector-binding-admission --clusterrole=neuvector-binding-admission --serviceaccount=neuvector:default
kubectl create clusterrole neuvector-binding-customresourcedefinition --verb=watch,create,get --resource=customresourcedefinitions
kubectl create clusterrolebinding  neuvector-binding-customresourcedefinition --clusterrole=neuvector-binding-customresourcedefinition --serviceaccount=neuvector:default
kubectl create clusterrole neuvector-binding-nvsecurityrules --verb=list,delete --resource=nvsecurityrules,nvclustersecurityrules
kubectl create clusterrolebinding neuvector-binding-nvsecurityrules --clusterrole=neuvector-binding-nvsecurityrules --serviceaccount=neuvector:default
kubectl create clusterrolebinding neuvector-binding-view --clusterrole=view --serviceaccount=neuvector:default
kubectl create rolebinding neuvector-admin --clusterrole=admin --serviceaccount=neuvector:default -n neuvector
```

检查是否有以下 RBAC 对象

```bash
kubectl get clusterrolebinding  | grep neuvector

neuvector-binding-admission                            ClusterRole/neuvector-binding-admission                            44h
neuvector-binding-app                                  ClusterRole/neuvector-binding-app                                  44h
neuvector-binding-customresourcedefinition             ClusterRole/neuvector-binding-customresourcedefinition             44h
neuvector-binding-nvadmissioncontrolsecurityrules      ClusterRole/neuvector-binding-nvadmissioncontrolsecurityrules      44h
neuvector-binding-nvsecurityrules                      ClusterRole/neuvector-binding-nvsecurityrules                      44h
neuvector-binding-nvwafsecurityrules                   ClusterRole/neuvector-binding-nvwafsecurityrules                   44h
neuvector-binding-rbac                                 ClusterRole/neuvector-binding-rbac                                 44h
neuvector-binding-view                                 ClusterRole/view                                                   44h
```

```bash
kubectl get rolebinding -n neuvector | grep neuvector

neuvector-admin         ClusterRole/admin            44h
neuvector-binding-psp   Role/neuvector-binding-psp   44h
```

部署 NeuVector

底层 Runtime 为 Docker

```ruby
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml
```

底层 Runtime 为 Containerd（对于 k3s 和 rke2 可以使用此 yaml 文件）

```ruby
kubectl apply -f https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-containerd-k8s.yaml
```

1.21 以下的 Kubernetes 版本会提示以下错误，将 yaml 文件下载将 batch/v1 修改为 batch/v1beta1

```less
error: unable to recognize "https://raw.githubusercontent.com/neuvector/manifests/main/kubernetes/5.0.0/neuvector-docker-k8s.yaml": no matches for kind "CronJob" in version "batch/v1"
```

1.20.x cronjob 还处于 beta 阶段，1.21 版本开始 cronjob 才正式 GA 。

默认部署 web-ui 使用的是 loadblance 类型的 Service，为了方便访问修改为 NodePort，也可以通过 Ingress 对外提供服务

```makefile
kubectl patch  svc neuvector-service-webui  -n neuvector --type='json' -p '[{"op":"replace","path":"/spec/type","value":"NodePort"},{"op":"add","path":"/spec/ports/0/nodePort","value":30888}]'
```

### 遇到的问题与解决
NeuVector 安装部署需要在k8s集群中部署包括controller，enforcer，manager，scanner在内的若干个pod，通过官方的yaml文件进行部署时，出现了上述的镜像拉取失败而导致的pod启动失败的问题。

![](./K8s(1)/err5.png)
![](./K8s(1)/err6.png)

如上图所示，多数pod启动失败，原因皆为镜像拉取失败导致的，查看官方提供的yaml文件，得知所有镜像的默认拉取策略为`Always`，尝试将其改为`IfNotPresent`，并提前手动拉取镜像(`docker pull`)到node节点本地来提高pod的部署与启动速度。

出现网络问题，这个应该就是之前镜像拉取失败的主要原因：
![](./K8s(1)/err3.png)
尝试配置DNS解析，并重启docker服务后解决：
![](./K8s(1)/err4.png)
所有的镜像均手动拉取下来后，重启之前启动失败的pod
![](./K8s(1)/succ0.png)
访问 `https://node-ip:30888`，使用 admin/admin登录
![](./K8s(1)/succ1.png)

### Trial
+ 查看容器存在的中高危漏洞情况
![](./K8s(1)/use0.png)
+ 查看容器内存使用与网络通信情况
![](./K8s(1)/use1.png)
+ 配置容器应用分组的相关进程规则，文件访问规则等
![](./K8s(1)/use2.png)
+ 配置集群与容器的网络规则
![](./K8s(1)/use3.png)
![](./K8s(1)/use5.png)
+ 查看集群中的安全事件
![](./K8s(1)/use4.png)
+ ...
